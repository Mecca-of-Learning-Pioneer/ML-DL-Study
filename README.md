# ML-DL-Study
ğŸ“˜MLPì—ì„œ ì§„í–‰í•˜ëŠ” ML, DL Studyì…ë‹ˆë‹¤.ğŸ“˜ 

## Algorithm

### Machine Learning(ML) - Structured Data

> #### Supervised Learning(ì§€ë„ í•™ìŠµ)
>
> - Classification(ë¶„ë¥˜) - Loss function : Cross-Entropy, Evaluation metrics : Accuracy (ë‹¤ë¥¸ ê²ƒ í›¨ì”¬ ë§ìŒ)
>   - k-NN
>     - ì–´ë–¤ ë°ì´í„°ì— ëŒ€í•œ ë‹µì„ êµ¬í•  ë•Œ, ì£¼ë³€ì˜ kê°œì˜ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„°ë¥¼ ë³´ê³  ë‹¤ìˆ˜ë¥¼ ì°¨ì§€í•˜ëŠ” ê²ƒì„ ì •ë‹µìœ¼ë¡œ íŒë‹¨
>   - Logistic Regression(ê¸°ë³¸ L2 Regularization)
>     - Binary Classification
>       - featureë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ì„ í˜• ë°©ì •ì‹ì„ ì–‘ì„± classì— ëŒ€í•´ì„œë§Œ í•™ìŠµ -> ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë„£ìœ¼ë©´, ì„  ìœ„ì˜ ê°’ì„ ë°˜í™˜(z) -> zê°’ì„ ì–‘ì„± class í™•ë¥ ë¡œ ë³€í™˜(1ì—ì„œ ëº€ ë‚˜ë¨¸ì§€ëŠ” ìŒì„± class í™•ë¥ )(sigmoid function)
>     - Multiclass Classification
>       - featureë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ì„ í˜• ë°©ì •ì‹ì„ ê° classë³„ë¡œ í•™ìŠµ -> ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë„£ìœ¼ë©´, ì„  ìœ„ì˜ ê°’ì„ ë°˜í™˜(z) -> zê°’ì„ classë³„ í™•ë¥ ë¡œ ë³€í™˜(softmax function)
>   - Decision Tree(Normalization í•„ìš”X)
>     - ì˜ˆ/ì•„ë‹ˆì˜¤ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì´ì–´ë‚˜ê°€ë©´ì„œ ì •ë‹µì„ ì°¾ì•„ í•™ìŠµ -> Leaf Nodeì—ì„œ ê°€ì¥ ë§ì€ classê°€ ì˜ˆì¸¡ class
>   - Ensemble Learning(ëŒ€ë¶€ë¶„ Decesion Tree ê¸°ë°˜)
>     - ë” ì¢‹ì€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ì—¬ëŸ¬ ê°œì˜ model í›ˆë ¨
>     - Random Forest, Extra Trees, Gradient Boosting, Histogram-based Gradient Boosting, (XGBoost) (Gradient Boosting ë°©ë²•ì€ Gradient Descentë¥¼ ì´ìš©í•œ ê²ƒ)
>
> - Regression(íšŒê·€) - Loss function : Mean Squared Error(MSE), Evaluation metrics : R-squared (ë‹¤ë¥¸ ê²ƒ í›¨ì”¬ ë§ìŒ)
>   - k-NN
>     - ì–´ë–¤ ë°ì´í„°ì— ëŒ€í•œ ë‹µì„ êµ¬í•  ë•Œ, ì£¼ë³€ì˜ kê°œì˜ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„°ì˜ ê°’ì„ í‰ê· ë‚¸ ê²ƒì„ ì •ë‹µìœ¼ë¡œ íŒë‹¨
>   - Linear Regression
>     - featureë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ì„ í˜• ë°©ì •ì‹ì„ í•™ìŠµ -> ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë„£ìœ¼ë©´, ì„  ìœ„ì˜ ê°’ì„ ë°˜í™˜(ì˜ˆì¸¡ ê°’)
>     - Ridge Regression
>       - Linear Regression + L2 Regularization
>     - Lasso Regression
>       - Linear Regression + L1 Regularization
>   - Decision Tree(Normalization í•„ìš”X)
>     - ì˜ˆ/ì•„ë‹ˆì˜¤ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì´ì–´ë‚˜ê°€ë©´ì„œ ì •ë‹µì„ ì°¾ì•„ í•™ìŠµ -> Leaf Nodeì— ë„ë‹¬í•œ ìƒ˜í”Œì˜ targetì„ í‰ê· í•œ ê°’ì´ ì˜ˆì¸¡ê°’
>   - Ensemble Learning(ëŒ€ë¶€ë¶„ Decesion Tree ê¸°ë°˜)
>     - ë” ì¢‹ì€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ì—¬ëŸ¬ ê°œì˜ model í›ˆë ¨
>     - Random Forest, Extra Trees, Gradient Boosting, Histogram-based Gradient Boosting, (XGBoost) (Gradient Boosting ë°©ë²•ì€ Gradient Descentë¥¼ ì´ìš©í•œ ê²ƒ)

> #### Unsupervised Learning


> #### Reinforcement Learning
> - RL Studyì—ì„œ ì§„í–‰

#### Tip!
- ë°˜ë“œì‹œ Normalizationì„ í•˜ê³  Regularization ì ìš©
- Model parameter : modelì´ featureì—ì„œ í•™ìŠµí•œ parameter
- Hyperparameter : ì‚¬ëŒì´ ì§€ì •í•˜ëŠ” parameter
- ì ì§„ì  í•™ìŠµ : Loss function ê°’ì„ ì ì  ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í›ˆë ¨í•˜ëŠ” í•™ìŠµë²•
  - Gradient Descent
    - Stochastic Gradient Descent
    - Minibatch Gradient Descent
    - Batch Gradient Descent
- Validation Set : hyperparameter íŠœë‹ì„ ìœ„í•´ modelì„ í‰ê°€í•  ë•Œ, test setë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ìœ„í•´ train setì—ì„œ ë‹¤ì‹œ ë–¼ì–´ ë‚¸ data set
- Test SetëŠ” model ì™„ì„± í›„ ë§ˆì§€ë§‰ì— í•œ ë²ˆë§Œ ì‚¬ìš©(ì—¬ëŸ¬ë²ˆ ì‚¬ìš© ì‹œ, modelì„ test setì— ë§ì¶”ëŠ” ê²ƒì´ê¸° ë•Œë¬¸)
  - k-fold Cross Validtaion : train setë¥¼ kê°œì˜ foldë¡œ ë‚˜ëˆˆ ë‹¤ìŒ í•œ foldê°€ validation setì˜ ì—­í• , ë‚˜ë¨¸ì§€ foldëŠ” model í›ˆë ¨ -> kë²ˆ ë°˜ë³µí•˜ì—¬ ì–»ì€ ëª¨ë“  validation scoreë¥¼ í‰ê· ëƒ„
- Hyperparameter tuning with AutoML
  - Grid Search : hyperparameter íƒìƒ‰(ê°’ì˜ ëª©ë¡ ì „ë‹¬) + cross validation
  - Random Search : hyperparameter íƒìƒ‰(ê°’ì˜ ë²”ìœ„ ì „ë‹¬) + cross validation
