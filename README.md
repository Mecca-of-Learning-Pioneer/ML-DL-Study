# ML-DL-Study
ğŸ“˜MLPì—ì„œ ì§„í–‰í•˜ëŠ” ML, DL Studyì…ë‹ˆë‹¤.ğŸ“˜ 

## Algorithm

### Machine Learning(ML) - Structured Data

> #### Supervised Learning(ì§€ë„ í•™ìŠµ)
>
> - Classification(ë¶„ë¥˜) - Loss function : Cross-Entropy, Evaluation metrics : Accuracy (ë‹¤ë¥¸ ê²ƒ í›¨ì”¬ ë§ìŒ)
>   - k-NN
>     - ì–´ë–¤ ë°ì´í„°ì— ëŒ€í•œ ë‹µì„ êµ¬í•  ë•Œ, ì£¼ë³€ì˜ kê°œì˜ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„°ë¥¼ ë³´ê³  ë‹¤ìˆ˜ë¥¼ ì°¨ì§€í•˜ëŠ” ê²ƒì„ ì •ë‹µìœ¼ë¡œ íŒë‹¨
>   - Logistic Regression(ê¸°ë³¸ L2 Regularization)
>     - Binary Classification(ì´ì§„ ë¶„ë¥˜)
>       - featureë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ì„ í˜• ë°©ì •ì‹ì„ ì–‘ì„± classì— ëŒ€í•´ì„œë§Œ í•™ìŠµ -> ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë„£ìœ¼ë©´, ì„  ìœ„ì˜ ê°’ì„ ë°˜í™˜(z) -> zê°’ì„ ì–‘ì„± class í™•ë¥ ë¡œ ë³€í™˜(1ì—ì„œ ëº€ ë‚˜ë¨¸ì§€ëŠ” ìŒì„± class í™•ë¥ )(sigmoid function)
>     - Multiclass Classification(ë‹¤ì¤‘ ë¶„ë¥˜)
>       - featureë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ì„ í˜• ë°©ì •ì‹ì„ ê° classë³„ë¡œ í•™ìŠµ -> ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë„£ìœ¼ë©´, ì„  ìœ„ì˜ ê°’ì„ ë°˜í™˜(z) -> zê°’ì„ classë³„ í™•ë¥ ë¡œ ë³€í™˜(softmax function)
>   - Decision Tree(Normalization í•„ìš”X)
>     - ì˜ˆ/ì•„ë‹ˆì˜¤ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì´ì–´ë‚˜ê°€ë©´ì„œ ì •ë‹µì„ ì°¾ì•„ í•™ìŠµ -> Leaf Nodeì—ì„œ ê°€ì¥ ë§ì€ classê°€ ì˜ˆì¸¡ class
>   - Ensemble Learning(ëŒ€ë¶€ë¶„ Decesion Tree ê¸°ë°˜)
>     - ë” ì¢‹ì€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ì—¬ëŸ¬ ê°œì˜ model í›ˆë ¨
>     - Random Forest, Extra Trees, Gradient Boosting, Histogram-based Gradient Boosting, (XGBoost) (Gradient Boosting ë°©ë²•ì€ Gradient Descentë¥¼ ì´ìš©í•œ ê²ƒ)
>
> - Regression(íšŒê·€) - Loss function : Mean Squared Error(MSE), Evaluation metrics : R-squared (ë‹¤ë¥¸ ê²ƒ í›¨ì”¬ ë§ìŒ)
>   - k-NN
>     - ì–´ë–¤ ë°ì´í„°ì— ëŒ€í•œ ë‹µì„ êµ¬í•  ë•Œ, ì£¼ë³€ì˜ kê°œì˜ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„°ì˜ ê°’ì„ í‰ê· ë‚¸ ê²ƒì„ ì •ë‹µìœ¼ë¡œ íŒë‹¨
>   - Linear Regression
>     - featureë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ì„ í˜• ë°©ì •ì‹ì„ í•™ìŠµ -> ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë„£ìœ¼ë©´, ì„  ìœ„ì˜ ê°’ì„ ë°˜í™˜(ì˜ˆì¸¡ ê°’)
>     - Ridge Regression
>       - Linear Regression + L2 Regularization
>     - Lasso Regression
>       - Linear Regression + L1 Regularization
>   - Decision Tree(Normalization í•„ìš”X)
>     - ì˜ˆ/ì•„ë‹ˆì˜¤ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì´ì–´ë‚˜ê°€ë©´ì„œ ì •ë‹µì„ ì°¾ì•„ í•™ìŠµ -> Leaf Nodeì— ë„ë‹¬í•œ ìƒ˜í”Œì˜ targetì„ í‰ê· í•œ ê°’ì´ ì˜ˆì¸¡ê°’
>   - Ensemble Learning(ëŒ€ë¶€ë¶„ Decesion Tree ê¸°ë°˜)
>     - ë” ì¢‹ì€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ì—¬ëŸ¬ ê°œì˜ model í›ˆë ¨
>     - Random Forest, Extra Trees, Gradient Boosting, Histogram-based Gradient Boosting, (XGBoost) (Gradient Boosting ë°©ë²•ì€ Gradient Descentë¥¼ ì´ìš©í•œ ê²ƒ)

> #### Unsupervised Learning(ë¹„ì§€ë„ í•™ìŠµ)
>
> - Clustering(êµ°ì§‘)
>   - k-Means
>     - clusterì˜ í‰ê· ê°’(cluster center/centroid)ì„ ëœë¤ìœ¼ë¡œ kê°œ ì •í•¨ -> [ê° ìƒ˜í”Œì´ ê°€ì¥ ê°€ê¹Œìš´ centroidì— ê°€ì„œ clusterë¥¼ ì´ë£¸ -> ë‹¤ì‹œ centroidë¥¼ êµ¬í•¨] -> centroidì— ë³€í™”ê°€ ì—†ì„ ë•Œê¹Œì§€ []êµ¬ê°„ ë°˜ë³µ -> kê°œì˜ clusterë¡œ ë¶„ë¥˜ë¨
>     - elbow ë°©ë²•ìœ¼ë¡œ ìµœì ì˜ kê°’ì„ ì°¾ì„ ìˆ˜ ìˆìŒ
> 
> - Dimensionality Reduction(ì°¨ì›ì¶•ì†Œ)
>   - Principal Component Analysis = PCA - Principal Component(ì£¼ì„±ë¶„) : ë°ì´í„°ì˜ íŠ¹ì§•ì„ ì˜ í‘œí˜„í•˜ëŠ” ì–´ë–¤ ë²¡í„°
>     - ì—¬ëŸ¬ ì£¼ì„±ë¶„ ì¤‘ ì¼ë¶€ ì£¼ì„±ë¶„ë§Œì„ ì„ íƒí•´ì„œ ë°ì´í„°ì˜ dimension(feature)ë¥¼ ì¤„ì„


> #### Reinforcement Learning(ê°•í™” í•™ìŠµ)
> - RL Studyì—ì„œ ì§„í–‰

#### Tip!
- ë°˜ë“œì‹œ Normalizationì„ í•˜ê³  Regularization ì ìš©
- Model parameter : modelì´ featureì—ì„œ í•™ìŠµí•œ parameter
- Hyperparameter : ì‚¬ëŒì´ ì§€ì •í•˜ëŠ” parameter
- ì ì§„ì  í•™ìŠµ : Loss function ê°’ì„ ì ì  ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í›ˆë ¨í•˜ëŠ” í•™ìŠµë²•
  - Gradient Descent
    - Stochastic Gradient Descent
    - Minibatch Gradient Descent
    - Batch Gradient Descent
- Validation Set : hyperparameter íŠœë‹ì„ ìœ„í•´ modelì„ í‰ê°€í•  ë•Œ, test setë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ìœ„í•´ train setì—ì„œ ë‹¤ì‹œ ë–¼ì–´ ë‚¸ data set
- Test SetëŠ” model ì™„ì„± í›„ ë§ˆì§€ë§‰ì— í•œ ë²ˆë§Œ ì‚¬ìš©(ì—¬ëŸ¬ë²ˆ ì‚¬ìš© ì‹œ, modelì„ test setì— ë§ì¶”ëŠ” ê²ƒì´ê¸° ë•Œë¬¸)
  - k-fold Cross Validtaion : train setë¥¼ kê°œì˜ foldë¡œ ë‚˜ëˆˆ ë‹¤ìŒ í•œ foldê°€ validation setì˜ ì—­í• , ë‚˜ë¨¸ì§€ foldëŠ” model í›ˆë ¨ -> kë²ˆ ë°˜ë³µí•˜ì—¬ ì–»ì€ ëª¨ë“  validation scoreë¥¼ í‰ê· ëƒ„
- Hyperparameter tuning with AutoML
  - Grid Search : hyperparameter íƒìƒ‰(ê°’ì˜ ëª©ë¡ ì „ë‹¬) + cross validation
  - Random Search : hyperparameter íƒìƒ‰(ê°’ì˜ ë²”ìœ„ ì „ë‹¬) + cross validation
- Dimensionality Reductionì„ í•˜ì—¬ ë°ì´í„°ì˜ dimension(feature)ë¥¼ ì¤„ì¸ ë’¤, ë‹¤ë¥¸ ML ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨í•  ìˆ˜ ìˆìŒ
- ë°ì´í„°ì˜ dimension(feature)ì„ 3ê°œ ì´í•˜ë¡œ ì¤„ì´ë©´, ì‹œê°í™”í•˜ê¸° ì¢‹ìŒ(3D or 2Dë¡œ í‘œí˜„ ê°€ëŠ¥)

### Deep Learning(DL) - Structured Data
> - DLì˜ ëª©í‘œ : loss functionì˜ ìµœì†Œê°’ ì°¾ê¸° = Optimization(ìµœì í™”)
> - ANN : Artificial Neural Network(ì¸ê³µ ì‹ ê²½ë§)
> - DNN : Deep Neural Network(ì‹¬ì¸µ ì‹ ê²½ë§)
> - SLP : Single Layer Perceptron(ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ )
> - MLP : Multi Layer Perceptron(ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ )
> - Node : ANNì„ êµ¬ì„±í•˜ëŠ” ëª¨ë“  featureë¥¼ nodeë¼ê³  ë¶€ë¦„

> - ANN âŠƒ DNN
> - ANN = { SLP, {DNN} }
> - DNN = { MLP, CNN, RNN }

> - Input Layer(ì…ë ¥ì¸µ)
> - Hidden Layer(ì€ë‹‰ì¸µ) âŠƒ FC(Fully Connected) Layer = Dense Layer(ë°€ì§‘ì¸µ)
> - Output Layer(ì¶œë ¥ì¸µ)

> - Activation Function(í™œì„±í™” í•¨ìˆ˜) : ê° nodeì˜ ì„ í˜•ë°©ì •ì‹ ê³„ì‚° ê²°ê³¼ì— ì ìš©ë˜ëŠ” í•¨ìˆ˜
>   - sigmoid function(ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜)
>   - softmax function(ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜)
>   - ReLU(ë ë£¨ í•¨ìˆ˜)
>   - tanh(í•˜ì´í¼ë³¼ë¦­ íƒ„ì  íŠ¸ í•¨ìˆ˜)

> - Optimizer : ìµœì†Œì˜ lossê°’ì„ ì°¾ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜
>   - SGD
>   - Adaptive Learning Rate(ì ì‘ì  í•™ìŠµë¥ ) ì‚¬ìš©í•˜ëŠ” optimizer - ëª¨ë¸ì´ ìµœì ì ì— ê°€ê¹Œì´ ê°ˆìˆ˜ë¡ í•™ìŠµë¥ ì„ ë‚®ì¶œ ìˆ˜ ìˆìŒ
>     - Adagrad
>     - RMSprop
>     - Adam : Momentum optimization + RMSprop

> - Dropout : hidden layerì— ìˆëŠ” ì¼ë¶€ nodeë¥¼ ë„ê³  í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒ - overfitting ë°©ì§€

> - ì¼ë°˜ Data : ìˆœì„œì— ì˜ë¯¸ê°€ ì—†ëŠ” data, ì˜ˆ) Image
> - Sequential Data(ìˆœì°¨ ë°ì´í„°) : ìˆœì„œì— ì˜ë¯¸ê°€ ìˆëŠ” data, ì˜ˆ) Text, Time Series(ì‹œê³„ì—´)(ì¼ì •í•œ ì‹œê°„ ê°„ê²©ìœ¼ë¡œ ê¸°ë¡ëœ ë°ì´í„°)
>   - sequence : í•˜ë‚˜ì˜ ìƒ˜í”Œ
>   - Text Data
>     - token : ë¶„ë¦¬ëœ ë‹¨ì–´
>     - ì–´íœ˜ ì‚¬ì „ : train setì—ì„œ ê³ ìœ í•œ ë‹¨ì–´ë¥¼ ë½‘ì•„ ë§Œë“  ëª©ë¡
>     - ë‹¨ì–´ë§ˆë‹¤ ê³ ìœ í•œ ì •ìˆ˜ë¥¼ ë¶€ì—¬í•´ ìˆ«ì ë°ì´í„°ë¡œ ë°”ê¿ˆ(0 : padding, 1 : ë¬¸ì¥ì˜ ì‹œì‘, 2 : ì–´íœ˜ ì‚¬ì „ì— ì—†ëŠ” toekn)
>     - ì •ìˆ˜ê°’ ì‚¬ì´ì—ëŠ” ì–´ë– í•œ ê´€ê³„ë„ ì—†ìŒ - One-hot encoding, Word embedding ì´ìš©

> - CNN(Convolution Neural Network)(í•©ì„±ê³± ì‹ ê²½ë§) - image data ì²˜ë¦¬ì— íŠ¹í™”ë˜ì–´ ìˆìŒ
>   - convolution layerì„ ê°€ì§€ê³  ìˆëŠ” NN
>   - filter = kernel => convolution layerì—ì„œ feature map ìƒì„±
>   - same padding : convolution ê³¼ì •ì„ ê±°ì¹˜ê³ ë„ output í¬ê¸°ë¥¼ inputê³¼ ë™ì¼í•˜ê²Œ í•˜ê¸° ìœ„í•¨
>   - stride : convolution ì—°ì‚° ê³¼ì •ì—ì„œ filterì˜ ì´ë™ í¬ê¸°
>   - pooling : convolution layerì—ì„œ ìƒì„±ëœ feature mapì˜ ê°€ë¡œì„¸ë¡œ í¬ê¸°ë¥¼ ì¤„ì„
>     - max pooling : filterë¥¼ ì°ì€ ì˜ì—­ì—ì„œ ê°€ì¥ í° ê°’ ê³ ë¥´ê¸°
>     - average pooling : filterë¥¼ ì°ì€ ì˜ì—­ì—ì„œ í‰ê· ê°’ ê³„ì‚°í•˜ê¸°

> - RNN(Recurrent Neural Network)(ìˆœí™˜ ì‹ ê²½ë§) - sequential data ì²˜ë¦¬ì— íŠ¹í™”ë˜ì–´ ìˆìŒ
>   - recurrent layerì„ ê°€ì§€ê³  ìˆëŠ” NN
>   - rucurrent layer = cell
>   - cellì˜ ì¶œë ¥ = hidden state
>   - timestep : ìƒ˜í”Œì„ ì²˜ë¦¬í•˜ëŠ” í•œ ë‹¨ê³„, text data - 1ê°œì˜ tokenì´ í•˜ë‚˜ì˜ timestep
>   - NLP(Natural Language Processing)(ìì—°ì–´ ì²˜ë¦¬)ì—ì„œ ì´ìš© - ìŒì„± ì¸ì‹, ê¸°ê³„ ë²ˆì—­, ê°ì„± ë¶„ì„ ë“±
>   - ì£¼ìš” Model
>     - LSTM(Long Short-Term Memory)
>     - GRU(Gated Recurrent Unit)

#### Tip!
- ì´ë¯¸ì§€ í”½ì…€ì€ 0~255 ì‚¬ì´ì˜ ì •ìˆ˜ê°’ì„ ê°€ì§ -> 255ë¡œ ë‚˜ëˆ„ì–´ 0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ normalization
- DLì—ì„œëŠ” cross validation ëŒ€ì‹  validation setì„ ë³„ë„ë¡œ ëœì–´ë‚´ì–´ ì‚¬ìš©
- ëª¨ë“  hidden layerì™€ output layerì—ëŠ” bias(ì ˆí¸)ê³¼ activation functionì´ ìˆìŒ(ë‹¨, Regressionì˜ ê²½ìš°ì—ëŠ” output layerì— activation functionì´ ì—†ìŒ)
- Binary Classificationì„ í•  ë•Œ output layerì—ì„œëŠ” sigmoid function ì‚¬ìš©
- Multiclass Classificationì„ í•  ë•Œ output layerì—ì„œëŠ” softmax function ì‚¬ìš©
- ë³´í†µ convolution layerì™€ pooling layerì€ ê±°ì˜ í•­ìƒ í•¨ê»˜ ì‚¬ìš©
